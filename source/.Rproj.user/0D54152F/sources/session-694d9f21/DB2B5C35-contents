## The Price of Civility: Economic and Social Returns on Investment in Toxicity Moderation

_Meta_ estimates its users create approximately [one billion](https://web.archive.org/web/20240531061745/https://www.facebook.com/business/news/stories-can-do-it) stories—ephemeral posts—daily. Stylistically, each additional post augments the appeal of platforms like Meta, TikTok, and X because it contributes potentially novel and diverse content to a massive content pool. To tame the scale of these content pools and to ensure that its size indeed augments a platform's appeal, one of the platforms' most critical functions is to distribute the most relevant content per user.^[In addition to facilitating the content production and consumption, the core of a platform’s business is to distribute the content to its consumers. Due to the unprecedented scale of content production, social media platforms cannot expose all of its users to all the available content. Instead, they curate personalized subsets of content. In contrast to traditional media outlets, where editors select content for a broad audience, recommender systems on these platforms generate tailored lists of content for individual users.] Well-designed content recommendations tend to positively affect the duration of user engagement [@AridorEtAl_2024], which is crucial for platforms because advertisers are interested in capturing users' attention and pay these platforms to gain a share of it. This is so significant that digital advertising, a substantial portion of which is placed on social media, now accounts for most of global advertising expenditures [@DeisenrothEtAl_2024].

Formally, a social platform chooses a targeting rule that picks, for each user $i$, a personalized subset $\mathbf{x}_i$ from the total pool of posts to show the user. A post is characterized by a vector of characteristics, $x \in \mathbb{R}^K$ which can include, for example, toxicity or sentiment expressed. Platforms choose the posts that maximize the revenue-weighted time spent $(t_i(\mathbf{x}_i))$ on the platform. $\alpha(\mathbf{x}_i)$ represents the monetary gains the platform gets per unit of time spent from showing $\mathbf{x}_i$ to $i$ [@AridorEtAl_2024, p. 3]. In addition, the platform faces costs $c(\mathbf{N}, \mathbf{M})$ that increases in the total numbers of users $\mathbf{N}$ and posts $\mathbf{M}$. These may include the technical infrastructure as well as labor costs.

$$
\max_{\{\mathbf{x}_i\} \subseteq U; \mathbf{x}_j^p} \left( \sum_i \alpha(\mathbf{x}_i) t_i(\mathbf{x}_i) - c(\mathbf{N}, \mathbf{M}) \right)
$$

One specific driver of costs is content moderation: Despite selecting potentially relevant content per user, platforms also have to filter content that should _not_ be displayed to any user. Because platforms cannot control the content production directly, they typically set rules that define which type of content shall (not) be displayed. However, actively moderating the content production by enforcing these rules and sanctioning violations is costly because platforms need to design algorithms that flag potentially unwanted content and employ human moderators who evaluate the flagged posts. These are considered _direct_ costs and represented by $c(\mathbf{N}, \mathbf{M})$.

In addition, there may be _indirect_ costs because field experimental evidence suggests that the removal of harmful content^[This includes hate speech and misinformation, for instance, and is considered harmful as the literature works with the assumption that such content imposes negative externalities on groups of the population [@BeknazarYuzbashevEtAl_2022, p. 8]] reduces content consumption. Hence, and perhaps counterintuitively, $t(\cdot)$ decreases in $\mathbf{x}_i$'s civility. However, anecdotal evidence suggests that advertisers react to harmful content by withdrawing their campaign budget for platforms that promote such content. In November 2023, CNN reported that at least a dozen major brands, such as IBM or Disney, halted their ad spending over concerns about antisemitism and hate speech on X. Such _brand safety_ measures indicate that $\alpha(\cdot)$ may increase in $\mathbf{x}_i$'s civility. Moreover, @AhmadEtAl_2024 find that most brand managers prefer their advertisements not to be displayed on websites that distribute misinformation and lack civility.

Focusing on one specific form of uncivil content, it is not clear how the product of $\alpha(\cdot)t(\cdot)$ is affected by toxicity. This study aims to find out.